{
  "version": "0.1.0",
  "model_list": [
    {
      "model_id": "Llama-3.2-1B-Instruct-q4f16_0-MLC",
      "model_lib": "llama_q4f16_0_103234372b479fcb179199b6d1b3d127",
      "model_url": "https://huggingface.co/mlc-ai/Llama-3.2-1B-Instruct-q4f16_0-MLC",
      "estimated_vram_bytes": 879040000,
      "model_type": "llama",
      "quantization": "q4f16_0",
      "context_window_size": 131072,
      "prefill_chunk_size": 8192,
      "description": "Fast and efficient 1B parameter LLaMA model optimized for mobile devices",
      "model_config": {
        "hidden_size": 2048,
        "intermediate_size": 8192,
        "num_attention_heads": 32,
        "num_hidden_layers": 16,
        "rms_norm_eps": 1e-05,
        "vocab_size": 128256,
        "tie_word_embeddings": true,
        "position_embedding_base": 500000.0,
        "rope_scaling": {
          "factor": 32.0,
          "high_freq_factor": 4.0,
          "low_freq_factor": 1.0,
          "original_max_position_embeddings": 8192,
          "rope_type": "llama3"
        },
        "num_key_value_heads": 8,
        "head_dim": 64,
        "tensor_parallel_shards": 1,
        "pipeline_parallel_stages": 1,
        "max_batch_size": 128,
        "temperature": 0.6,
        "presence_penalty": 0.0,
        "frequency_penalty": 0.0,
        "repetition_penalty": 1.0,
        "top_p": 0.9
      }
    }
  ],
  "runtime_config": {
    "device_type": "android",
    "use_gpu": true,
    "gpu_memory_fraction": 0.8,
    "enable_batching": true,
    "max_concurrent_requests": 1,
    "cache_size_mb": 512,
    "model_cache_dir": "/data/data/com.example.offline_ai_companion/files/mlc_models",
    "temp_dir": "/data/data/com.example.offline_ai_companion/cache/mlc_temp"
  },
  "features": {
    "streaming_response": true,
    "gpu_acceleration": true,
    "memory_optimization": true,
    "fast_model_loading": true,
    "model_switching": true
  }
}
